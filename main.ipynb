{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvopGw7dfXe5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6201a0f-cbcc-472b-de0c-492df7242c7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls \"/content/drive/MyDrive\""
      ],
      "metadata": {
        "id": "mZ7WpsXRfZ98",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "884337a7-86eb-41e9-a7df-ef4b720573f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'210_44K22.2_CP2_baocao - Trang Nguyễn Thị Minh.gdoc'\n",
            " abbreviations.xlsx\n",
            " Bontry.gsheet\n",
            " Bontry.xlsx\n",
            " Classroom\n",
            "'Colab Notebooks'\n",
            "'DASHBOARD DATACO.twbx'\n",
            "'ĐỀ CƯƠNG SƠ BỘ CAPSTONE PROJECT1.gdoc'\n",
            " hasaki7000.xlsx\n",
            " HASAKIHEHEHE.sql\n",
            " OTHERS_MODEL.joblib\n",
            " PACKAGING_MODEL.joblib\n",
            " PRICE_MODEL.joblib\n",
            " QUALITY_MODEL.joblib\n",
            " SERVICE_MODEL.joblib\n",
            " STORE_MODEL.joblib\n",
            " tokenizer.pkl\n",
            "'TranVanLoc_HoLeKhoiNguyen (3).zip'\n",
            " vietnamese-stopwords-1.txt\n",
            "'vietnamese-stopwords (1).txt'\n",
            " Visualize\n",
            " word2vec_sentiment.model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.23.5 pandas==1.5.3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mBkyql77-f3_",
        "outputId": "dd64283a-f103-49b1-ac86-a5f6de3580f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy==1.23.5 in /usr/local/lib/python3.11/dist-packages (1.23.5)\n",
            "Collecting pandas==1.5.3\n",
            "  Downloading pandas-1.5.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.11/dist-packages (from pandas==1.5.3) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas==1.5.3) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.1->pandas==1.5.3) (1.17.0)\n",
            "Downloading pandas-1.5.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m89.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pandas\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 1.5.3 which is incompatible.\n",
            "dask-expr 1.1.21 requires pandas>=2, but you have pandas 1.5.3 which is incompatible.\n",
            "bigframes 2.4.0 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n",
            "tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, but you have tensorflow 2.13.0 which is incompatible.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\n",
            "db-dtypes 1.4.3 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n",
            "dask-cudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "mizani 0.13.5 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "pymc 5.22.0 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\n",
            "xarray 2025.3.1 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "xarray 2025.3.1 requires pandas>=2.1, but you have pandas 1.5.3 which is incompatible.\n",
            "plotnine 0.14.5 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "cudf-cu12 25.2.1 requires pandas<2.2.4dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pandas-1.5.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Embedding, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.models import load_model\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from joblib import dump, load\n",
        "from sklearn.preprocessing import label_binarize"
      ],
      "metadata": {
        "id": "XYkPAwYUfbdI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "0038317b-d875-4197-c56c-f997d0fc345d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-4ab0e0af450f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_libs\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhashtable\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_hashtable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlib\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_lib\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtslib\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_tslib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_err\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pragma: no cover\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0m_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/_libs/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_libs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterval\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInterval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m from pandas._libs.tslibs import (\n\u001b[1;32m     15\u001b[0m     \u001b[0mNaT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/_libs/interval.pyx\u001b[0m in \u001b[0;36minit pandas._libs.interval\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "!pip install pyvi\n",
        "from pyvi import ViTokenizer\n",
        "!pip install emoji\n",
        "import emoji\n",
        "# Load các thành phần đã lưu\n",
        "w2v_model = Word2Vec.load('/content/drive/MyDrive/word2vec_sentiment.model')\n",
        "tokenizer_load = load('/content/drive/MyDrive/tokenizer.pkl')\n",
        "QUALITY_model = load('/content/drive/MyDrive/QUALITY_MODEL.joblib')\n",
        "SERVICE_model = load('/content/drive/MyDrive/SERVICE_MODEL.joblib')\n",
        "OTHERS_model = load('/content/drive/MyDrive/OTHERS_MODEL.joblib')\n",
        "STORE_model = load('/content/drive/MyDrive/STORE_MODEL.joblib')\n",
        "PACKAGING_model = load('/content/drive/MyDrive/PACKAGING_MODEL.joblib')\n",
        "PRICE_model = load('/content/drive/MyDrive/PRICE_MODEL.joblib')\n",
        "\n",
        "# Tiền xử lý\n",
        "def preprocess_text(text, abbreviation_dict, stopwords):\n",
        "    # Kiểm tra nếu input không phải là chuỗi, trả về chuỗi rỗng\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    # 1. Chuyển về chữ thường\n",
        "    text = text.lower()\n",
        "    # 2. Giải mã chữ viết tắt\n",
        "    for abbreviation, meaning in abbreviation_dict.items():\n",
        "        text = re.sub(r'\\b' + re.escape(abbreviation) + r'\\b', meaning, text)\n",
        "    # 3. Chuyển biểu tượng cảm xúc thành văn bản\n",
        "    text = emoji.demojize(text)\n",
        "    # 4. Loại bỏ ký tự đặc biệt\n",
        "    text = re.sub(r\"[!@#$[]()]\", \"\", text)\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    # 5. Tách từ và loại bỏ stopwords\n",
        "    tokenized_text = ViTokenizer.tokenize(text)\n",
        "    tokens = [word for word in tokenized_text.split() if word not in stopwords]\n",
        "    # 6. Trả về danh sách các từ đã xử lý\n",
        "    return tokens\n",
        "\n",
        "# Đọc danh sách chữ viết tắt\n",
        "abbreviation_path = \"/content/drive/MyDrive/abbreviations.xlsx\"\n",
        "abbs_df = pd.read_excel(abbreviation_path, engine='openpyxl')\n",
        "abbreviation_dict = dict(zip(abbs_df['abbreviation'].astype(str), abbs_df['meaning']))\n",
        "\n",
        "# Đọc danh sách stopwords\n",
        "stopwords_path = \"/content/drive/MyDrive/vietnamese-stopwords (1).txt\"\n",
        "with open(stopwords_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    stopwords = f.read().split(\"\\n\")\n",
        "\n",
        "# Hàm chuyển đổi dữ liệu sang vector đặc trưng bằng Word2Vec\n",
        "def vectorize_comment(comment, model, vector_dim):\n",
        "    words = comment\n",
        "    word_vectors = [model.wv[word] for word in words if word in list(w2v_model.wv.index_to_key)]\n",
        "    if len(word_vectors) == 0:\n",
        "        return np.zeros(vector_dim)  # Nếu không có từ nào trong mô hình Word2Vec\n",
        "    return np.mean(word_vectors, axis=0)\n",
        "\n",
        "\n",
        "# Pipeline dự đoán\n",
        "def predict_sentiment(input_text):\n",
        "    processed_text_RF = preprocess_text(input_text,abbreviation_dict, stopwords)\n",
        "    processed_text_NN = preprocess_text(input_text,abbreviation_dict, stopwords)\n",
        "    # Ghép các từ đã token hóa thành một chuỗi duy nhất\n",
        "    processed_text_joined = \" \".join(processed_text_NN)\n",
        "    # Chuyển đổi sang chuỗi số bằng tokenizer\n",
        "    new_sequences = tokenizer_load.texts_to_sequences([processed_text_joined])  # Đảm bảo đầu vào là danh sách chuỗi\n",
        "    # Chuyển dữ liệu thành padded sequences\n",
        "    max_length = 100\n",
        "    new_padded = pad_sequences(new_sequences, maxlen=max_length, padding='post', truncating='post')\n",
        "\n",
        "    input_vector = vectorize_comment(processed_text_RF, w2v_model, vector_dim=150)\n",
        "\n",
        "    QUALITY = QUALITY_model.predict([input_vector])[0]\n",
        "    SERVICE_pred = SERVICE_model.predict(new_padded)\n",
        "    SERVICE = np.argmax(SERVICE_pred)\n",
        "    OTHERS = OTHERS_model.predict([input_vector])[0]\n",
        "    STORE = STORE_model.predict([input_vector])[0]\n",
        "    PACKAGING = PACKAGING_model.predict([input_vector])[0]\n",
        "    PRICE_pred = PRICE_model.predict(new_padded)\n",
        "    PRICE = np.argmax(PRICE_pred)\n",
        "    return reverse_label_mapping[QUALITY],reverse_label_mapping[SERVICE],reverse_label_mapping[OTHERS],reverse_label_mapping[STORE],reverse_label_mapping[PACKAGING],reverse_label_mapping[PRICE]\n",
        "\n",
        "# Mapping nhãn\n",
        "label_mapping = {\"positive\": 2, \"neutral\": 1, \"negative\": 0}\n",
        "reverse_label_mapping = {v: k for k, v in label_mapping.items()}\n",
        "\n",
        "# Ví dụ\n",
        "input_text = \"Tôi thấy sản phẩm này cũng dùng được, bạn tư vấn rất nhiệt tình tuy nhiên bao bì hơi xấu cần cải thiện nội dung trên bao bì\"\n",
        "QUALITY, SERVICE, OTHERS, STORE, PACKAGING, PRICE = predict_sentiment(input_text)\n",
        "print(f\"Input: {input_text}\")\n",
        "print(f\"QUALITY: {QUALITY}\")\n",
        "print(f\"SERVICE: {SERVICE}\")\n",
        "print(f\"OTHERS: {OTHERS}\")\n",
        "print(f\"STORE: {STORE}\")\n",
        "print(f\"PACKAGING: {PACKAGING}\")\n",
        "print(f\"PRICE: {PRICE}\")"
      ],
      "metadata": {
        "id": "XI4uxXVYfc8N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}